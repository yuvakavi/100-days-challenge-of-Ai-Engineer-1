import numpy as np

# 1️⃣ Sample data (hours studied, hours slept)
X = np.array([
    [2, 9],
    [1, 5],
    [3, 6],
    [4, 8]
])

# Labels (0 = fail, 1 = pass)
y = np.array([[0], [0], [1], [1]])

# Normalize input (important for faster learning)
X = X / np.amax(X, axis=0)

# 2️⃣ Define activation function (sigmoid)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# 3️⃣ Initialize weights & biases randomly
np.random.seed(42)
weights_input_hidden = np.random.uniform(size=(2, 3))
weights_hidden_output = np.random.uniform(size=(3, 1))
bias_hidden = np.random.uniform(size=(1, 3))
bias_output = np.random.uniform(size=(1, 1))

# 4️⃣ Training loop
lr = 0.1  # learning rate
epochs = 10000

for i in range(epochs):
    # Forward Pass
    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden
    hidden_output = sigmoid(hidden_input)
    
    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output
    final_output = sigmoid(final_input)
    
    # Compute error
    error = y - final_output
    
    # Backpropagation
    d_output = error * sigmoid_derivative(final_output)
    error_hidden = d_output.dot(weights_hidden_output.T)
    d_hidden = error_hidden * sigmoid_derivative(hidden_output)
    
    # Update weights and biases
    weights_hidden_output += hidden_output.T.dot(d_output) * lr
    weights_input_hidden += X.T.dot(d_hidden) * lr
    bias_output += np.sum(d_output, axis=0, keepdims=True) * lr
    bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * lr

# 5️⃣ Final Output
print("\nPredicted Output after Training:")
print(final_output.round(2))
