# 🚀 XGBoost Example – Manual Titanic-like Data

import pandas as pd
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# -----------------------------
# 🧩 Step 1: Create Manual Data
# -----------------------------
# Features:
# Pclass = Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)
# Sex    = 0 Male, 1 Female
# Age    = Age in years
# Fare   = Ticket Fare
# SibSp  = Number of siblings/spouses aboard
# Survived = Target (1 = Survived, 0 = Not survived)

data = {
    'Pclass': [1, 3, 2, 3, 1, 3, 2, 1, 3, 2],
    'Sex':    [1, 0, 1, 0, 1, 0, 0, 1, 0, 1],
    'Age':    [29, 35, 22, 40, 30, 18, 28, 45, 36, 24],
    'SibSp':  [0, 1, 0, 1, 1, 0, 0, 0, 1, 0],
    'Fare':   [80, 7, 15, 8, 90, 6, 20, 85, 7, 25],
    'Survived':[1,0,1,0,1,0,0,1,0,1]
}

df = pd.DataFrame(data)
print("Manual Titanic-like Data:\n", df)

# -----------------------------
# ⚙️ Step 2: Split Features & Target
# -----------------------------
X = df[['Pclass','Sex','Age','SibSp','Fare']]
y = df['Survived']

# Split train/test (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -----------------------------
# 🚀 Step 3: Train XGBoost Model
# -----------------------------
model = XGBClassifier(
    n_estimators=50,
    learning_rate=0.1,
    max_depth=3,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

model.fit(X_train, y_train)

# -----------------------------
# 🔍 Step 4: Predict & Evaluate
# -----------------------------
y_pred = model.predict(X_test)

print("\n✅ Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
