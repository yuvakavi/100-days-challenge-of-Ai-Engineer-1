!pip install transformers datasets accelerate -q

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# -------------------------
# 1. Load tiny model
# -------------------------
model_name = "sshleifer/tiny-gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
policy_model = AutoModelForCausalLM.from_pretrained(model_name)

optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-5)

# -------------------------
# 2. Reward function
# -------------------------
polite_words = ["please", "thank", "sorry", "help", "welcome"]

def reward_function(text):
    return sum([w in text.lower() for w in polite_words])

# -------------------------
# 3. Tiny RL training loop
# -------------------------
prompts = [
    "Say hello politely.",
    "Greet the user nicely.",
    "Respond to someone kindly."
]

for epoch in range(20):
    total_loss = 0

    for prompt in prompts:
        # Encode prompt
        inputs = tokenizer(prompt, return_tensors="pt")

        # Forward pass WITH gradient
        output = policy_model(**inputs, labels=inputs["input_ids"])

        # Log likelihood loss (negative)
        logprob_loss = output.loss

        # Generate sample reply (no gradients here)
        with torch.no_grad():
            sample_ids = policy_model.generate(inputs["input_ids"], max_new_tokens=20)
            text = tokenizer.decode(sample_ids[0])

        # Compute reward
        reward = reward_function(text)

        # RL loss = - reward * logprob
        rl_loss = logprob_loss * (-reward)

        rl_loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += rl_loss.item()

    print(f"Epoch {epoch} | RL Loss: {total_loss:.4f}")

print("\nTraining finished!")
